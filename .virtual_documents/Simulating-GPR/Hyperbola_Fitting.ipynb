import math
import matplotlib.pyplot as plt
import gprsim
import numpy as np


def visualize_preconditioning():
    fig, ax = plt.subplots(3, 3, figsize=(18,15))
    
    for SNR, i in zip([10, 0.05, 0.009], range(0,3)):
        # Parameters
        eps_r = 3                  # relative dielectric permittivity
        rf = 40e6                 # radar frequency different from pulse frequency
        dt = 1e-9                  # seconds
        dx = 1                     # meters
        region_shape = (70, 1e-6)  # grid (x,z)
        wavetype = 'gaussian'
        
        # Point reflectors at (x,t) where x [m] and t [s]
        reflectors = [(35, 50e-9)]
        
        data, x_positions, t_samples = gprsim.gprsim(eps_r, rf, dt, dx, reflectors, region_shape, wavetype, SNR)
        
        im = ax[i][0].imshow(data, aspect='auto', cmap='seismic')
        if i == 2:
            ax[i][0].set_xlabel("Antenna position (m)")
        ax[i][0].set_ylabel("Time (ns)")
        ax[i][0].set_title(f"Simulated Radargram: SNR={SNR}")
        cbar = fig.colorbar(im, label="Amplitude")
        
        t, x = data.argsort(axis=0)[::-1,:][0], np.arange(0, data.shape[1]) * dx 
        x_window, t_window = gprsim.minimize_tracewise_slope_window(data, 20, 1, 5)
        x_greedy, t_greedy = gprsim.minimize_tracewise_slope_greedy(data, 15, dx)
        x_dynamic, t_dynamic, c, b = gprsim.smooth_path_dp(data, dx, lam=0.001)
        
        ax[i][1].set_title(f"Data Preconditioning Using n-max Signals SNR={SNR}")
        im1 = ax[i][1].imshow(data, aspect='auto', extent=[0,70,-1000,0], cmap='seismic')
        ax[i][1].scatter(x, -t, label="Max signal at each trace")
        ax[i][1].plot(x_window, -t_window, label="informed greedy", c='black', linestyle='-')
        ax[i][1].plot(x_greedy, -t_greedy, label="simple greedy", c='black', linestyle='--')
        ax[i][1].plot(x_dynamic, t_dynamic, label="memoized cost", c='black', linestyle=':')
        ax[i][1].legend(loc="lower right")
    
    
        im2 = ax[i][2].imshow((c-c.min(axis=0))/(c.max(axis=0)-c.min(axis=0)), aspect='auto', cmap='RdYlGn')
        ax[i][2].set_title("Normalized Score Matrix (memoized cost method \n -- accounting for slope and intensity)")
visualize_preconditioning()





eps_r = 3                  # relative dielectric permittivity
rf = 400e6                 # radar frequency different from pulse frequency
dt = 1e-9                  # seconds
dx = 1                     # meters
region_shape = (70, 1e-6)  # grid (x,z)
wavetype = 'gaussian'
SNR = 0.02

# Point reflectors at (x,t) where x [m] and t [s]
reflectors = [(35, 50e-9)]

data, x_positions, t_samples = gprsim.gprsim(eps_r, rf, dt, dx, reflectors, region_shape, wavetype, SNR)

fig, ax = plt.subplots(2, figsize = (7,5))
fig.tight_layout(h_pad=4)

for n in range(0,4):
    t = data.argsort(axis=0)[::-1,:][n]* -1 # Ttake the indices of the max 5 signals
    x = np.arange(0, data.shape[1]) * dx # 
    ax[0].scatter(x, t, label=f"n={n}")

sample_signal = data[:,35]
ax[1].plot(np.sort(sample_signal)[::-1], c='black')
ax[1].set_title("sorted signal magnitudes at x = 35 (zero offset)")
ax[1].set_xlabel("t (ns)")
ax[0].legend()
ax[0].set_title("n maximum signal along each trace ")
ax[0].set_xlabel("x distance (m)")





# Parameters
eps_r = 3                  # relative dielectric permittivity
rf = 400e6                 # radar frequency different from pulse frequency
dt = 1e-9                  # seconds
dx = 1                     # meters
region_shape = (70, 1e-6)  # grid (x,z)
wavetype = 'gaussian'
# Point reflectors at (x,t) where x [m] and t [s]
reflectors = [(35, 50e-9)]
x_0, t_0 = reflectors[0][0], reflectors[0][1]

drs = []
dx0s = []
dt0s = []
for SNR in np.arange(0.0009,0.2,0.002):
    data, x_positions, t_samples = gprsim.gprsim(eps_r, rf, dt, dx, reflectors, region_shape, wavetype, SNR)
    v, z, x0, t0 = gprsim.fit_hyperbola(data, 1, 'robust_fit', dx, dt, lam=0.001)
    drs.append(((np.abs(eps_r-(3e8/v)**2)) / ((np.abs(eps_r+(3e8/v)**2))/2))*100)
    dx0s.append(((np.abs(x_0-x0)) / ((np.abs(x_0+x0)/2)))*100)
    dt0s.append(((np.abs(t_0-t0)) / ((np.abs(t_0+t0)/2)))*100)
        


plt.scatter(np.arange(0.0009,0.2,0.002), drs); plt.title("% Difference in recovered eps_r (averaged over 5 runs)"); plt.xlabel("SNR"); plt.ylabel("% difference"); plt.xscale('log'); plt.yscale('log'); 



from gprsim import smooth_path_dp
from gprsim import fit_hyperbola
fig, ax = plt.subplots(1,4, figsize=(18,4))
# Parameters
eps_r = 3                  # relative dielectric permittivity
rf = 400e6                 # radar frequency different from pulse frequency
dt = 1e-9                  # seconds
dx = 1                     # meters
nx = 70
nt = 1e-6
region_shape = (nx, nt)  # grid (x,z)
wavetype = 'gaussian'
SNR = 0.01

# Point reflectors at (x,t) where x [m] and t [s]
reflectors = [(35, 50e-9)]

data, x_positions, t_samples = gprsim.gprsim(eps_r, rf, dt, dx, reflectors, region_shape, wavetype, SNR)
data = data[0:400, 20:50]
# im = ax[0].imshow(c-c.mean(axis=0), aspect="auto")
# im = ax[0].imshow(data, aspect="auto", cmap='binary', alpha=0.4)
# im2 = ax[1].imshow(data, aspect="auto", cmap='binary', alpha=0.4)
# im3 = ax[2].imshow(data, aspect="auto", cmap='binary', alpha=0.4)

'''
Plot the intensity only case
'''
x, t, c, b = smooth_path_dp(data, dx, lam=0) # Intensity only path
ax[0].imshow((c-c.min(axis=0))/(c.max(axis=0)-c.min(axis=0)),cmap='binary', aspect='auto')
v, z, x0, t0 = fit_hyperbola(data, 1, 'robust_fit', dx, dt, x, t)
ax[0].plot(x, (2*np.sqrt((x-x0)**2 + z**2))/(v*1e-9), c='tab:red', ls='--', lw=2)

'''
Plot the slope only case
'''
x, t, c, b = smooth_path_dp(data, dx, lam=1)
ax[1].imshow((c-c.min(axis=0))/(c.max(axis=0)-c.min(axis=0)),cmap='binary', aspect='auto')
v, z, x0, t0 = fit_hyperbola(data, 1, 'robust_fit', dx, dt, x, t)
ax[1].plot(x, (2*np.sqrt((x-x0)**2 + z**2))/(v*1e-9), c='tab:blue', ls='--', lw=2)

'''rfd
plot the optimal case
'''
# x, t, c, b = smooth_path_dp(data, dx, lam=0.001)
# ax[2].imshow((c-c.min(axis=0))/(c.max(axis=0)-c.min(axis=0)),cmap='binary', aspect='auto')
# v, z, x0, t0 = fit_hyperbola(data, 1, 'robust_fit', dx, dt, 0.001)
# ax[2].plot(x, (2*np.sqrt((x-x0)**2 + z**2))/(v*1e-9), c='y', ls='--', lw=2)
x, t = gprsim.minimize_tracewise_slope_window(data, 100, dx, 5)
v, z, x0, t0 = fit_hyperbola(data, 1, 'robust_fit', dx, dt, x, t)
ax[2].imshow((c-c.min(axis=0))/(c.max(axis=0)-c.min(axis=0)),cmap='binary', aspect='auto')
ax[2].plot(x, (2*np.sqrt((x-x0)**2 + z**2))/(v*1e-9), c='y', ls='--', lw=2)


ax[3].imshow(data, aspect="auto", cmap='seismic')




import numpy as np
import matplotlib.pyplot as plt
import gprsim
from scipy.interpolate import interp1d

eps_r = 3                  # relative dielectric permittivity
rf = 400e6                 # radar frequency different from pulse frequency
dt = 1e-9                  # seconds
dx = 1                     # meters
Xlim = 20
Tlim = 1e-7
region_shape = (Xlim, Tlim)  # grid (x,z)
Nx, Nt = int(Xlim/dx), int(Tlim/dt)
wavetype = 'gaussian'
SNR = 100

# Point reflectors at (x,t) where x [m] and t [s]
reflectors = [(7, 20e-9), (8, 18e-9), (9, 19e-9), (10, 20e-9), (11, 21e-9), (12, 22e-9), (13, 23e-9), (14, 24e-9), (15, 25e-9), (16, 26e-9)]

data, x_positions, t_samples = gprsim.gprsim(eps_r, rf, dt, dx, reflectors, region_shape, wavetype, SNR)


fig, ax = plt.subplots(1, 2, figsize = (10, 5))
ax[0].imshow(data, aspect="auto")
# --- Velocity model (constant) ---
c0 = 3e8
v = c0 / np.sqrt(eps_r)

# --- Output migration grid ---
x_img = x_positions.copy()       # same as input x
z_img = np.linspace(0, v*t_max/2, Nt) 
# converting time window to max depth: z_max = v * t_max / 2

migrated = np.zeros((len(x_img), len(z_img)))

# --- Migration Loop ---
for ix, x in enumerate(x_img):
    for iz, z in enumerate(z_img):

        # Travel time curve T(x_s; x,z) for all traces
        # Two-way travel time in constant velocity
        T = 2.0 * np.sqrt((x_positions - x)**2 + z**2) / v

        # Only use times inside the data window
        valid = (T >= t_samples[0]) & (T <= t_samples[-1])
        if not np.any(valid):
            continue
            
        # Interpolate each trace’s data at time T
        amplitudes = np.zeros_like(T)
        for isrc in np.where(valid)[0]:
            f = interp1d(t_samples, data[:, isrc], kind='linear', fill_value=0, bounds_error=False)
            amplitudes[isrc] = f(T[isrc])

        # Sum contributions (simplest Kirchhoff)
        migrated[ix, iz] = np.sum(amplitudes)

# --- Plot result ---
ax[1].imshow(migrated.T, aspect="auto", extent=[x_img.min(), x_img.max(), z_img.max(), z_img.min()])
ax[1].set_title("Kirchhoff Migrated Section")
ax[1].set_xlabel("Distance (m)")
ax[1].set_ylabel("Depth (m)")



pip install pandas


import pandas as pd

# Load the CSV file into a pandas DataFrame
import csv

# Reading a CSV file into a list of lists
dates_t = []
tornados = []
with open('/Users/coltenrodriguez/Downloads/data (4).csv', 'r', newline='') as csvfile:
    reader = csv.reader(csvfile)
    i = 0
    for row in reader:
        if i>2:
            dates_t.append(row[0])
            tornados.append(row[1])
        i+=1
        


fig = plt.figure(figsize=(10,7))
dates_t = np.array(dates_t)
def parse(s):
    if '*' in s:
        return int(s[:-1])
    return int(s)

tornados_by_month = np.array([parse(count) for count in tornados[np.where(dates_t == '198701')[0][0]:]])
dates = np.array([count for count in dates_t[np.where(dates_t == '198701')[0][0]:]])
# Convert the cumulative count at each month to the month count
tornados_by_month
def cumulative_to_monthly(cumul):
    monthly = []
    for i in range(len(cumul)):
        
        # January detection: either index 0, or cumulative reset
        if i == 0 or cumul[i] < cumul[i - 1]:
            monthly.append(cumul[i])
        else:
            monthly.append(cumul[i] - cumul[i - 1])
    
    return monthly
# print([c for d,c in zip(dates, cumulative_to_monthly(tornados_by_month))])
def monthly_to_cumulative(monthly, original_cumulative):
    cumul = []
    running = 0

    for i, m in enumerate(monthly):
        # Detect January using the original cumulative structure
        if i == 0 or original_cumulative[i] < original_cumulative[i - 1]:
            running = m      # January starts fresh
        else:
            running += m
    
        cumul.append(running)

    return cumul
    
plt.bar(range(len(dates)), cumulative_to_monthly(tornados_by_month))
def three_month_running_mean(monthly):
    n = len(monthly)
    result = [None] * n
    
    for i in range(n):
        # Determine position in yearly cycle
        month_index = i % 12  # 0 = Jan, ..., 11 = Dec
        
        if month_index == 0:  # January
            vals = monthly[i:i+2]          # Jan–Feb
        elif month_index == 11:  # December
            vals = monthly[i-1:i+1]        # Nov–Dec
        else:
            vals = monthly[i-1:i+2]        # centered 3-month mean
        
        result[i] = sum(vals) / len(vals)
    
    return result
    
plt.scatter(range(len(dates)), three_month_running_mean(cumulative_to_monthly(tornados_by_month)))
print(len(dates))


fig = plt.figure(figsize=(9,5))
plt.plot(np.arange(1987, 2024, 1), [int(n) for n in tornados[37:-1]])
plt.ylabel('Year')
plt.xlabel('Number of Hurricanes')
plt.title('Number of hurricanes per year in \n the contiguous U.S from SPC')


m = {'Ja':1/12, 'F':2/12, 'Ma':3/12, 'Ap':4/12, 'M':5/12, 'J':6/12, 'Jl':7/12, 'A':8/12, 'S':9/12, 'O':10/12, 'N':11/12, 'D':12/12}
def get_date(s):
    month = s[1]

    if month == 'A':
        if 'AM' in s:
            month = 'Ap'
    if month == 'M':
        if 'MA' in s:
            month = 'Ma'
    if month == 'J':
        if 'JA' in s:
            month = 'Jl'
        elif 'JF' in s:
            month = 'Ja'
    # print(month, m[month])
    return m[month]
      
   
fig = plt.figure(figsize=(9,5))
text_file = open("/Users/coltenrodriguez/Downloads/oni.ascii.txt", "r")
lines = text_file.read().split('  ')
dates = np.array([float(date[-4:])+get_date(str(date[:-5])) for date in lines[4::3]][444:])
y = np.array([float(num[:-2]) for num in lines[6::3]][444:])
text_file.close()
plt.plot(dates, y, c='black', alpha=0.5)
plt.scatter(dates[y>=0], y[y>=0], color="red")
plt.scatter(dates[y<0],  y[y<0],  color="blue")
print(len(dates))


x1 = np.arange(1987, 2025, 1)
y1 = [int(n) for n in tornados[37:]]      # Primary axis
x2 = dates
y2 = y      # Secondary axis
fig, ax1 = plt.subplots(figsize=(9,5))

# Plot on first y-axis
ax1.bar(x1, y1, lw=10, color='gray')
ax1.set_xlabel("date")
ax1.set_ylabel("# Tornados (yearly total)")
ax1.tick_params(axis='y', labelcolor="gray")

# Create second y-axis sharing the same x-axis
ax2 = ax1.twinx()
ax2.scatter(dates[y>=0], y[y>=0], color="red", s=15)
ax2.scatter(dates[y<0],  y[y<0],  color="blue", s=15)
ax2.plot(x2, y2, c='black', alpha=0.5)
ax2.set_ylabel("Nino 3.4 SST anom ($^oC$)", color="red")
ax2.tick_params(axis='y', labelcolor="red")

plt.title("Number of tornados per year in the contiguous U.S from SPC \n with overlayed ENSO period")
plt.show()



x2 = dates
y2 = y      # Secondary axis
fig, ax1 = plt.subplots(figsize=(9,5))

# Plot on first y-axis
t_running_mean = three_month_running_mean(cumulative_to_monthly(tornados_by_month))
ax1.bar(x2, three_month_running_mean(cumulative_to_monthly(tornados_by_month)), width=0.1, color='gray')
ax1.set_xlabel("date")
ax1.set_ylabel("# Tornados (3 month running mean)")
ax1.tick_params(axis='y', labelcolor="gray")

# Create second y-axis sharing the same x-axis
ax2 = ax1.twinx()
ax2.scatter(dates[y>=0], y[y>=0], color="red", s=15)
ax2.scatter(dates[y<0],  y[y<0],  color="blue", s=15)
ax2.plot(x2, y2, c='black', alpha=0.5)
ax2.set_ylabel("Nino 3.4 SST anom ($^oC$)", color="red")
ax2.tick_params(axis='y', labelcolor="red")

plt.title("Number of tornados (3 month running mean) in the contiguous U.S from SPC \n with overlayed ENSO period")
plt.show()


def compute_climatology(monthly):
    return [np.mean(monthly[n::12]) for n in range(0,12)]
def compute_anom(means, c):
    anoms = []
    for i, mean in enumerate(means):
        month = i%12
        climatology=c[month]
        anoms.append(mean-climatology)
    return anoms
climatologies = compute_climatology(t_running_mean)
anoms = compute_anom(t_running_mean, climatologies)


x2 = dates
y2 = y      # Secondary axis
fig, ax1 = plt.subplots(figsize=(12,5))

# Plot on first y-axis
ax1.bar(x2, anoms, width=0.1, color='gray')
ax1.set_xlabel("date")
ax1.set_ylabel("Anomaly in 3 month running mean of # tornados")
ax1.tick_params(axis='y', labelcolor="gray")

# Create second y-axis sharing the same x-axis
ax2 = ax1.twinx()
ax2.scatter(dates[y>=0], y[y>=0], color="red", s=15)
ax2.scatter(dates[y<0],  y[y<0],  color="blue", s=15)
ax2.plot(x2, y2, c='black', alpha=0.5)
ax2.set_ylabel("Nino 3.4 SST anom ($^oC$)", color="red")
ax2.tick_params(axis='y', labelcolor="red")

plt.title("Tornado count anomaly (3 month running mean) in the contiguous U.S from SPC \n with overlayed ENSO period")
plt.show()


plt.scatter(y, anoms)
plt.xlabel('Nino 3.4 SST anomaly ($^oC$)')
plt.ylabel('tornado count anomaly')
plt.title('Relation Between ENSO Period and Tornado Count Anomaly \n (both from 3 month means)')


def GPA(grades):
    weights = {'A+': 4.0, 'A': 4.0, 'A-': 3.7, 'B+': 3.3, 'B': 3.0, 'B-': 2.7, 'C+': 2.3}
    sum = 0
    for grade in grades:
        sum+=weights[grade]
    return sum/len(grades)
        

grades_all = ['A', 'B', 'A', 'A-', 'A-', 'A', 'B-', 'A', 'B+', 'A-', 'A', 'B', 'A', 'A', 'A', 'A', 'B+', 'A', 'A-', 'A', 'A', 'A', 'A-', 'A-']
print(GPA(grades_all))

grades_adv = ['B+', 'A-', 'A', 'B', 'A', 'A', 'A', 'A', 'B+', 'A', 'A-', 'A', 'A', 'A', 'A-', 'A-']
print(GPA(grades_adv))

grades_maj = ['A-', 'A', 'A-', 'A', 'A-', 'A-']
print(GPA(grades_maj))


import numpy as np

def sec(theta):
    return 1/(np.cos(theta))
    
def transmission_angles(er, h, xq, zq, xa):
    # theta_gv: 0 → pi/2, with 2^11 points
    theta_gv = np.linspace(0, np.pi/2, 2**11)

    # theta_av = asin(min(sqrt(er)*sin(theta_gv), 1))
    # np.minimum applies elementwise
    inside = np.sqrt(er) * np.sin(theta_gv)
    inside = np.minimum(inside, 1.0)
    theta_av = np.arcsin(inside)

    # Compute absolute error array exactly like MATLAB
    diff = np.abs(np.abs(xa - xq) - zq * np.tan(theta_gv) - h * np.tan(theta_av))

    # Find index of minimum
    ind_min = np.argmin(diff)

    # Return angles
    theta_a = theta_av[ind_min]
    theta_g = theta_gv[ind_min]

    return theta_a, theta_g

def kirchoff_migration_2D(B, h, er, t, xA, xQ, zQ):
    # B: preprocessed B-scan
    # h: Antenna height
    # er: relative permitivity
    # t = time vector
    # zA: spatial vecotr (sampling)

    # output I: image, claculated as xQ x zQ rectangle , abs[I], rows should be depth, cols should be xQ

    c = 3e8 # speed of light in a vac
    Nt = t.shape[0]
    NxQ = xQ.shape[0]
    NzQ = zQ.shape[0]
    NxA = xA.shape[0]
    
    # Initialize the kernel
    kernel = np.zeros((NxA, NxQ, NxQ))
    print(kernel.shape)

    # calculate the half derivative
    Fs = None # sampling frequency
    f = None   

    # B_f = np.fft.fftshift(np.fft.fft(B, 0), 0) # f-x domain
    # D12 = np.tile(np.sqrt(1j * 2 * np.pi * f.T), (1,NxA)) # half derivative operator
    # dB = np.real(np.ifft(np.fft.fftshift(D12*B_f, 1), 1))
    dB = B

    for m in range(NxQ):
        xq = xQ[m]
        for n in range(NzQ):
            zq = zQ[n]
            theta_av = np.zeros((NxA, 1))
            theta_gv = np.zeros((NxA, 1))
            for p in range(NxA):
                (theta_a, theta_g) = transmission_angles(er, h, xq, zq, xA[p])
                theta_av[p] = theta_a
                theta_gv[p] = theta_g
            r = h * sec(theta_av) + zq * sec(theta_gv)
            t_exps = 2*(h*sec(theta_av/c) + zq*sec(theta_gv)/(c*np.sqrt(er)))

            # select the correct theta from dB
            # Trace out the hyp
            # replicate MATLAB repmat(t, [NxA, 1])
            t_rep = np.tile(t, (NxA, 1))      # shape: (NxA, len(t))
            
            # find index of minimum along axis=1 (MATLAB dim=2)
            inds = np.argmin(np.abs(t_rep - t_exps), axis=1)
            
            # convert (row, col) pairs → linear indices like MATLAB sub2ind
            rows = inds                     # row indices
            cols = np.arange(1, NxA+1) - 1  # MATLAB 1:NxA → Python 0:NxA-1
            
            # linearized index into dB
            lin_inds = rows * dB.shape[1] + cols
            
            # construct the kernel
            print((np.cos(theta_av)/np.sqrt(r))
            kernel[:, m, n] = np.cos(theta_av)/np.sqrt(r) * dB[lin_inds]

    I = np.abs(np.trapz(kernelm, x=zA, axis=0)).squeeze().T
    return I

    
    
# Read in B-scan data
B = data
meta = None

x = np.arange(nx) # positions of scans
t = np.arange(nt) # time

# perform preprocessing
BKGR = B-B.mean(axis=0)

# No filtering
BKGR_filtered = BKGR

# perform kirchoff migration
h = 0.1 # Height of the antenna
er = 3.0 # rel permitivity
xQ = x
zQ = np.arange(0, 0.4, 5e-3) # Why a 5mm increment?
I = kirchoff_migration_2D(BKGR_filtered, h, er, t, x, xQ, zQ)
plt.imshow(I, apect='auto')


pip install "laspy[lazrs,laszip]"


data = {'Riveria': {
    '2002': {
        'x': [],
        'y': []},
    '2016': {
        'x': [],
        'y': []},
    },
        'Cottons': {
    '2002': {
        'x': [],
        'y': []},
    '2016': {
        'x': [],
        'y': []},
    },
        'Barb-wire': {
    '2002': {
        'x': [],
        'y': []},
    '2016': {
        'x': [],
        'y': []},
    },
        'Uppers': {
    '2002': {
        'x': [],
        'y': []},
    '2016': {
        'x': [],
        'y': []},
    },
        'Lowers': {
    '2002': {
        'x': [],
        'y': []},
    '2016': {
        'x': [],
        'y': []},
    }
}



import laspy

path = "/Users/coltenrodriguez/Downloads/20020522_scripps_333750_ld_p19.copc.laz"
las = laspy.read(path)

print(las)


x = las.x[::100]
y = las.y[::100]
z = las.z[::100]


intensity = las.intensity
classification = las.classification
return_num = las.return_number


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(x, y, s=1)
plt.show()
print(min(y), max(y))


import numpy as np
from shapely.geometry import LineString, Point
import laspy
from pyproj import Transformer
import numpy as np

paths = ["/Users/coltenrodriguez/Downloads/20020522_scripps_333750_ld_p19.copc.laz"]
# las_src = laspy.read(src)
# src_crs = las_src.header.parse_crs()
paths = ["/Users/coltenrodriguez/Downloads/20160405_11SMS44553693.copc.laz", "/Users/coltenrodriguez/Downloads/20160405_11SMS44403696.copc.laz", "/Users/coltenrodriguez/Downloads/20160405_11SMS44403694.copc.laz", "/Users/coltenrodriguez/Downloads/20160405_11SMS44403693.copc.laz", "/Users/coltenrodriguez/Downloads/20160405_11SMS44253696.copc.laz", "/Users/coltenrodriguez/Downloads/20160405_11SMS44253694.copc.laz"]
paths = ["/Users/coltenrodriguez/Downloads/2009_20091021_ncmp_ca_024_113_223026_geoclassified.copc.laz", "/Users/coltenrodriguez/Downloads/2009_20091021_ncmp_ca_023_113_222050_geoclassified.copc.laz"]
x, y, z = [], [], []
for path in paths:
    las = laspy.read(path)
    # print(f'{src_crs}, \n \n {las.header.parse_crs()}')
    # transformer = Transformer.from_crs(src_crs, las.header.parse_crs(), always_xy=True)
    # x_shift, y_shift = transformer.transform(las.x[::1000], las.y[::1000])
    # x+=list(x_shift)
    # y+=list(y_shift)
    x+=las.x[::10]
    y+=las.y[::10]
    z+=las.z[::10]


plt.scatter(x, y)


import numpy as np

# transects = {'Riveria': {'P1': np.array([442434.6,3696739.4]), 'P2': np.array([443232.1,3697292.6])},
#              'Cottons': {'P1': np.array([444118.7,3694580.6]), 'P2': np.array([444817.0,3695228.5])},
#              'Barb-wire': {'P1': np.array([444272.2,3694187.2]), 'P2': np.array([445074.8,3694985.2])},
#              'Uppers': {'P1': np.array([444672.8,3693212.2]), 'P2': np.array([445580.0,3694659.9])},
#              'Lowers': {'P1': np.array([445699.0,3693250.8]), 'P2':np.array([446056.1,3694254.6])}}

transects = {'Riveria': {'P1': np.array([1000624.796,3709534.613]), 'P2': np.array([1001391.465,3710134.938])},
             'Cottons': {'P1': np.array([1002436.267,3707470.483]), 'P2': np.array([1003098.040,3708159.742])},
             'Barb-wire': {'P1': np.array([1002612.739,3707085.406]), 'P2': np.array([1003370.384,3707931.062])},
             'Uppers': {'P1': np.array([1003070.48,3706132.41]), 'P2': np.array([1003895.0328,3707634.4632])},
             'Lowers': {'P1': np.array([1004095.786,3706230.124]), 'P2':np.array([1004395.2719,3707256.1592])}}

# transects = {'Riveria': {'P1': np.array([33.40844003,-117.61909132]), 'P2': np.array([33.413471609,-117.610549861])},
#              'Cottons': {'P1': np.array([33.389057886,-117.600846299]), 'P2': np.array([33.394937546,-117.593377963])},
#              'Barb-wire': {'P1': np.array([33.385517709,-117.599171411]), 'P2': np.array([33.392756557,-117.590591152])},
#              'Uppers': {'P1': np.array([33.376744579,-117.594804685]), 'P2': np.array([33.389848238,-117.585139608])},
#              'Lowers': {'P1': np.array([33.377145242,-117.583775214]), 'P2':np.array([33.386216702,-117.579996361])}}


for transect in transects.keys():
    region = transect
    P1 = transects[region]['P1']
    P2 = transects[region]['P2']
    buffer_dist = 10.0   # meters on each side
    
    pts = np.vstack([x, y]).T   # shape (N,2)
    
    line_vec = P2 - P1
    line_len = np.linalg.norm(line_vec)
    line_unit = line_vec / line_len
    
    # Vector from P1 to every point
    p1_to_pts = pts - P1
    
    # Perpendicular distance to line (using 2D cross product magnitude)
    dist = np.abs(np.cross(line_unit, p1_to_pts))
    
    mask = dist <= buffer_dist
    x_sel = np.array(x)[mask]
    y_sel = np.array(y)[mask]
    z_sel = np.array(z)[mask]
    p1_to_pts_sel = p1_to_pts[mask]
    
    s_along = np.dot(p1_to_pts_sel, line_unit)   # distance along transect

    seg_mask = (s_along >= 0) & (s_along <= line_len)
    s_along = s_along[seg_mask]
    z_sel = z_sel[seg_mask]

    data2[region]['2009']['x'], data2[region]['2009']['y'] = s_along[z_sel != np.max(z_sel)], z_sel[z_sel != np.max(z_sel)]


# data[region]['2002']['x'], data[region]['2002']['y'] = s_along[z_sel != np.max(z_sel)], z_sel[z_sel != np.max(z_sel)]


fix, ax = plt.subplots(5,1, figsize=(15,13))
plt.title("Can Clemente Shore Profiles Time-Series")
plt.tight_layout()
lims = [(500, 750), (200, 450), (200, 450), (700, 950), (400, 650)]
sl = 0
mhw = 2.2

for i, region in enumerate(transects.keys()):
    # ax[i].scatter(np.array(data[region]['2002']['x']), data[region]['2002']['y'], s=1)
    # ax[i].scatter(np.array(data[region]['2016']['x']), data[region]['2016']['y'], s=1)
    idx_sort_2002 = np.argsort(np.array(data2[region]['2002']['x']))
    idx_sort_2016 = np.argsort(np.array(data2[region]['2016']['x']))
    idx_sort_2009 = np.argsort(np.array(data2[region]['2009']['x']))
    ax[i].scatter(np.array(data2[region]['2002']['x'])[idx_sort_2002], data2[region]['2002']['y'][idx_sort_2002], color="blue", label="2016", s=5)
    ax[i].scatter(np.array(data2[region]['2016']['x'])[idx_sort_2016], data2[region]['2016']['y'][idx_sort_2016],  color="red", label="2002 May", s=5)
    ax[i].scatter(np.array(data2[region]['2009']['x'])[idx_sort_2009], data2[region]['2009']['y'][idx_sort_2009],  color="black", label="2009 May", s=5)
    
    
    # plot the relevant sea levels
    ax[i].axhline(y=sl, color='gray', linestyle='--', linewidth=1)
    ax[i].axhline(y=mhw, color='gray', linewidth=1, label='MHW (from Webb et. al. 2023)')

    
    ax[i].set_title(region)
    if i == 4:
        ax[i].set_xlabel("distance")
    # ax[i].set_xlim(200, 450)
    ax[i].set_ylim(-2, 20)


im = cv2.cvtColor(cv2.imread("/Users/coltenrodriguez/Downloads/coastsat-image.jpg"), cv2.COLOR_BGR2RGB)


plt.imshow(im)


cols, rows, c = im.shape
for i in range(cols):
    for j in range(rows):
        val = im[i, j]
        if (220 <= val[0] <= 255) and (220 <= val[1] <= 255) and (220 <= val[2] <= 255):
            im[i, j] = [242, 227, 200]


plt.imshow(im)


cv2.imwrite("/Users/coltenrodriguez/Downloads/Diag_crossplot-nowhite.png", cv2.cvtColor(im, cv2.COLOR_BGR2RGB))


im = cv2.cvtColor(cv2.imread("/Users/coltenrodriguez/Downloads/2009_NCMP_CA_24_BareEarth_1mGrid.tif", cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB)
np.max(im)
